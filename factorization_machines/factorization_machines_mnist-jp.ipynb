{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Factorization Machines with MNIST\n",
    "_**Making a Binary Prediction of Whether a Handwritten Digit is a 0**_\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "  1. [Permissions and environment variables](#Permissions-and-environment-variables)\n",
    "  2. [Data ingestion](#Data-ingestion)\n",
    "  3. [Data inspection](#Data-inspection)\n",
    "  4. [Data conversion](#Data-conversion)\n",
    "3. [Training the FM model](#Training-the-FM-model)\n",
    "4. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "  1. [Import model into hosting](#Import-model-into-hosting)\n",
    "  2. [Create endpoint configuration](#Create-endpoint-configuration)\n",
    "  3. [Create endpoint](#Create-endpoint)\n",
    "5. [Validate the model for use](#Validate-the-model-for-use)\n",
    "\n",
    "---\n",
    "# MNISTを使ったFactorization Machine入門\n",
    "\n",
    "1. [はじめに]（＃はじめに）\n",
    "2. [前提条件と前処理]（＃前提条件と前処理）\n",
    "   1. [パーミッションと環境変数]（＃パーミッションと環境変数）\n",
    "   2. [データの取り込み]（＃データの取り込み）\n",
    "   3. [データ検査]（＃データ検査）\n",
    "   4. [データ変換]（＃データ変換）\n",
    "3. [FMモデルのトレーニング]（＃Training-the-FMモデル）\n",
    "4. [モデルのホスティングを設定する]（＃Set-up-the-model）\n",
    "   1. [モデルをホスティングにインポート]（＃インポートモデル - ホスティング）\n",
    "   2. [エンドポイント構成の作成]（＃Create-endpoint-configuration）\n",
    "   3. [エンドポイントの作成]（＃Create-endpoint）\n",
    "5. [使用するモデルの検証]（＃使用するモデルの検証）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to our example introducing Amazon SageMaker's Factorization Machines Algorithm!  Today, we're analyzing the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset which consists of images of handwritten digits, from zero to nine.  We'll use the individual pixel values from each 28 x 28 grayscale image to predict a yes or no label of whether the digit is a 0 or some other digit (1, 2, 3, ... 9).\n",
    "\n",
    "The method that we'll use is a factorization machine binary classifier.  A factorization machine is a general-purpose supervised learning algorithm that you which can use for both classification and regression tasks.  It is an extension of a linear model that is designed to parsimoniously capture interactions between features in high dimensional sparse datasets.  For example, in a click prediction system, the factorization machine model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category.  Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.\n",
    "\n",
    "Amazon SageMaker's Factorization Machine algorithm provides a robust, highly scalable implementation of this algorithm, which has become extremely popular in ad click prediction and recommender systems.  The main purpose of this notebook is to quickly show the basics of implementing Amazon SageMaker Factorization Machines, even if the use case of predicting a digit from an image is not where factorization machines shine.\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on.\n",
    "\n",
    "---\n",
    "## はじめに\n",
    "Amazon SageMakerのFactorization Machinesアルゴリズムの導入例へようこそ！今日、私たちは、0から9までの手書き数字の画像で構成される[MNIST]（https://en.wikipedia.org/wiki/MNIST_database）データセットを分析しています。各28×28グレースケール画像の個々のピクセル値を使用して、数字が0か別の数字（1,2,3、... 9）かどうかのはい/いいえのラベルを予測します。\n",
    "\n",
    "私たちが使用する方法は、分解マシンバイナリ分類器です。因数分解装置は、分類作業と回帰作業の両方に使用できる汎用の監視学習アルゴリズムです。これは、高次元の疎なデータセット内のフィーチャ間の相互作用を間違いなくキャプチャするように設計された線形モデルの拡張です。例えば、クリック予測システムでは、因数分解機械モデルは、ある広告カテゴリの広告が特定のページカテゴリのページに配置されたときに観察されるクリック率パターンを取得することができる。ファクトリゼーションマシンは、クリック予測やアイテムの推奨など、高次元の疎なデータセットを扱うタスクに適しています。\n",
    "\n",
    "Amazon SageMakerのFactorization Machineアルゴリズムは、このアルゴリズムの堅牢でスケーラブルな実装を提供します。このアルゴリズムは、広告のクリック予測や推薦システムで非常に人気が高くなっています。このノートブックの主な目的は、画像から数字を予測するユースケースが分解マシンが輝く場所ではなくても、Amazon SageMaker分解マシンの実装の基礎を素早く示すことです。\n",
    "\n",
    "まず、権限、構成など、いくつかの前提条件を設定して環境をセットアップする必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites and Preprocessing\n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s).\n",
    "\n",
    "---\n",
    "## 前提条件と前処理\n",
    "### パーミッションと環境変数\n",
    "\n",
    "_このノートブックは、ml.m4.xlargeノートブック・インスタンスで作成およびテストされました。\n",
    "\n",
    "以下を指定して始めましょう。\n",
    "\n",
    "- トレーニングとモデルデータに使用するS3バケットとプレフィックス。 これは、ノートブックインスタンス、トレーニング、およびホスティングと同じ領域内にある必要があります。\n",
    "- IAMの役割は、トレーニングを行い、データへのアクセスをホストするために使用されます。 これらを作成する方法については、ドキュメントを参照してください。 ノートブックインスタンス、トレーニング、および/またはホスティングに複数の役割が必要な場合は、boto regexpを適切な完全IAMロールのarn文字列に置き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Bucket(name='gluon-recommend-314676777416')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "#REGION = 'us-east-1' #N. Varvinia\n",
    "REGION = 'ap-northeast-1' #N. Varvinia\n",
    "sts = boto3.client('sts')\n",
    "bucket = 'gluon-recommend-{}'.format(sts.get_caller_identity()['Account'])\n",
    "s3 = boto3.resource('s3', region_name=REGION)\n",
    "s3.create_bucket(Bucket=bucket, CreateBucketConfiguration={'LocationConstraint': REGION})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "#bucket = 'sagemaker-shokout'\n",
    "prefix = 'sagemaker/DEMO-fm-mnist'\n",
    " \n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next, we read the dataset from an online URL into memory, for preprocessing prior to training. This processing could be done *in situ* by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### データ摂取\n",
    "\n",
    "次に、訓練の前に前処理のために、オンラインURLからデータセットをメモリに読み込みます。 この処理は、Amazon Athena、Amazon EMRのApache Spark、Amazon Redshiftなどで、インサイツ*で行うことができます。データセットが適切な場所にあると仮定します。 次に、トレーニングに使用するためにデータをS3に転送することです。 このような小さなデータセットの場合、メモリへの読み込みは大規模ではありませんが、大規模なデータセットの場合はそうです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 816 ms, sys: 278 ms, total: 1.09 s\n",
      "Wall time: 2.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle, gzip, numpy, urllib.request, json\n",
    "\n",
    "# Load the dataset\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection\n",
    "\n",
    "Once the dataset is imported, it's typical as part of the machine learning process to inspect the data, understand the distributions, and determine what type(s) of preprocessing might be needed. You can perform those tasks right here in the notebook. As an example, let's go ahead and look at one of the digits that is part of the dataset.\n",
    "\n",
    "### データ検査\n",
    "\n",
    "データセットをインポートすると、データを検査し、その分布を理解し、必要な事前処理のタイプを判断するために、マシン学習プロセスの一部として典型的です。 これらのタスクはノートブックですぐに実行できます。 例として、データセットの一部である数字の1つを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (2,10)\n",
    "\n",
    "\n",
    "def show_digit(img, caption='', subplot=None):\n",
    "    if subplot==None:\n",
    "        _,(subplot)=plt.subplots(1,1)\n",
    "    imgr=img.reshape((28,28))\n",
    "    subplot.axis('off')\n",
    "    subplot.imshow(imgr, cmap='gray')\n",
    "    plt.title(caption)\n",
    "\n",
    "show_digit(train_set[0][30], 'This is a {}'.format(train_set[1][30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data conversion\n",
    "\n",
    "Since algorithms have particular input and output requirements, converting the dataset is also part of the process that a data scientist goes through prior to initiating training. In this particular case, the Amazon SageMaker implementation of Factorization Machines takes recordIO-wrapped protobuf, where the data we have today is a pickle-ized numpy array on disk.\n",
    "\n",
    "Most of the conversion effort is handled by the Amazon SageMaker Python SDK, imported as `sagemaker` below.\n",
    "\n",
    "_Notice, despite the fact that most use cases for factorization machines will utilize spare input, we are writing our data out as dense tensors.  This will be fine since the MNIST dataset is not particularly large or high dimensional._\n",
    "\n",
    "---\n",
    "\n",
    "### データ変換\n",
    "\n",
    "アルゴリズムには特定の入出力要件があるため、データセットを変換することは、データ科学者がトレーニングを開始する前に行うプロセスの一部でもあります。 この特定のケースでは、ファクトライゼーションマシンのAmazon SageMaker実装ではrecordIO-wrapped protobufが使用されます。今日のデータは、ディスク上のpickle-numedアレイです。\n",
    "\n",
    "ほとんどの変換作業はAmazon SageMaker Python SDKによって処理され、以下の `sagemaker 'としてインポートされます。\n",
    "\n",
    "（注意）因子分解マシンのほとんどのユースケースがスペア入力を利用するという事実にもかかわらず、私たちは密なテンソルとしてデータを書き出しています。 これは、MNISTデータセットが特に大きかったり大きかったりしなかったのでうまくいくでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "vectors = np.array([t.tolist() for t in train_set[0]]).astype('float32')\n",
    "labels = np.where(np.array([t.tolist() for t in train_set[1]]) == 0, 1.0, 0.0).astype('float32')\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, vectors, labels)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload training data\n",
    "Now that we've created our recordIO-wrapped protobuf, we'll need to upload it to S3, so that Amazon SageMaker training can use it.\n",
    "\n",
    "## トレーニングデータをアップロードする\n",
    "recordIOラップされたprotobufを作成したので、S3にアップロードしてAmazon SageMakerのトレーニングで使用できるようにする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded training data location: s3://gluon-recommend-314676777416/sagemaker/DEMO-fm-mnist/train/recordio-pb-data\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "key = 'recordio-pb-data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also setup an output S3 location for the model artifact that will be output as the result of training with the algorithm.\n",
    "\n",
    "また、アルゴリズムを使ったトレーニングの結果として出力されるモデル成果物の出力S3の位置を設定してみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training artifacts will be uploaded to: s3://gluon-recommend-314676777416/sagemaker/DEMO-fm-mnist/output\n"
     ]
    }
   ],
   "source": [
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the factorization machine model\n",
    "\n",
    "Once we have the data preprocessed and available in the correct format for training, the next step is to actually train the model using the data. Since this data is relatively small, it isn't meant to show off the performance of the Amazon SageMaker's Factorization Machines in training, although we have tested it on multi-terabyte datasets.\n",
    "\n",
    "Again, we'll use the Amazon SageMaker Python SDK to kick off training and monitor status until it is completed.  In this example that takes between 7 and 11 minutes.  Despite the dataset being small, provisioning hardware and loading the algorithm container take time upfront.\n",
    "\n",
    "First, let's specify our containers.  Since we want this notebook to run in all 4 of Amazon SageMaker's regions, we'll create a small lookup.  More details on algorithm containers can be found in [AWS documentation](https://docs-aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html).\n",
    "\n",
    "## 因子分解モデルのトレーニング\n",
    "\n",
    "データを前処理し、トレーニング用の正しいフォーマットで利用できるようになると、次のステップはデータを使用して実際にモデルをトレーニングすることです。 このデータは比較的小さいので、Amazon SageMakerのファクタライゼーションマシンのトレーニングでのパフォーマンスを示すことを目的としたものではありませんが、数テラバイトのデータセットでテストしています。\n",
    "\n",
    "また、Amazon SageMaker Python SDKを使用してトレーニングを開始し、完了するまでステータスを監視します。 この例では、7〜11分かかります。 データセットが小さくても、ハードウェアのプロビジョニングとアルゴリズムコンテナのロードには時間がかかります。\n",
    "\n",
    "まず、コンテナを指定しましょう。 Amazon SageMakerの4つの地域すべてでこのノートブックを実行したいので、小さなルックアップを作成します。 アルゴリズムコンテナの詳細については、[AWS documentation]（https://docs-aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html）を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'factorization-machines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll kick off the base estimator, making sure to pass in the necessary hyperparameters.  Notice:\n",
    "- `feature_dim` is set to 784, which is the number of pixels in each 28 x 28 image.\n",
    "- `predictor_type` is set to 'binary_classifier' since we are trying to predict whether the image is or is not a 0.\n",
    "- `mini_batch_size` is set to 200.  This value can be tuned for relatively minor improvements in fit and speed, but selecting a reasonable value relative to the dataset is appropriate in most cases.\n",
    "- `num_factors` is set to 10.  As mentioned initially, factorization machines find a lower dimensional representation of the interactions for all features.  Making this value smaller provides a more parsimonious model, closer to a linear model, but may sacrifice information about interactions.  Making it larger provides a higher-dimensional representation of feature interactions, but adds computational complexity and can lead to overfitting.  In a practical application, time should be invested to tune this parameter to the appropriate value.\n",
    "\n",
    "---\n",
    "\n",
    "次に、必要なハイパーパラメータを確実に渡して、ベースエスティメータを開始します。通知：\n",
    "- `feature_dim`は784に設定され、これは各28 x 28画像のピクセル数です。\n",
    "- `predictor_type`は 'binary_classifier'に設定されています。画像が0であるかどうかを予測しようとしているからです。\n",
    "- `mini_batch_size`は200に設定されています。この値はフィットとスピードの比較的小さな改善のために調整できますが、ほとんどの場合、データセットに対して妥当な値を選択するのが適切です。\n",
    "- `num_factors`は10に設定されます。最初に述べたように、分解マシンはすべてのフィーチャのインタラクションのより低い次元表現を見つけます。この値を小さくすると、線形モデルに近いより簡潔なモデルが得られますが、相互作用に関する情報は犠牲になります。それを大きくすると、フィーチャのインタラクションの高次元表現が提供されますが、計算の複雑さが増し、オーバーフィットにつながる可能性があります。実際のアプリケーションでは、このパラメータを適切な値に調整するために時間を費やす必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: factorization-machines-2018-10-16-23-35-01-649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-16 23:35:01 Starting - Starting the training job...\n",
      "Launching requested ML instances......\n",
      "Preparing the instances for training...\n",
      "2018-10-16 23:37:00 Downloading - Downloading input data..."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(container,\n",
    "                                   role, \n",
    "                                   train_instance_count=1, \n",
    "                                   train_instance_type='ml.c4.xlarge',\n",
    "                                   output_path=output_location,\n",
    "                                   sagemaker_session=sess)\n",
    "fm.set_hyperparameters(feature_dim=784,\n",
    "                      predictor_type='binary_classifier',\n",
    "                      mini_batch_size=200,\n",
    "                      num_factors=10)\n",
    "\n",
    "fm.fit({'train': s3_train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hosting for the model\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint.  This will allow out to make predictions (or inference) from the model dyanamically.\n",
    "\n",
    "_Note, Amazon SageMaker allows you the flexibility of importing models trained elsewhere, as well as the choice of not importing models if the target of model creation is AWS Lambda, AWS Greengrass, Amazon Redshift, Amazon Athena, or other deployment target._\n",
    "\n",
    "---\n",
    "\n",
    "## モデルのホスティングを設定する\n",
    "モデルを訓練したので、Amazon SageMakerのリアルタイムホストエンドポイントの後ろに配置することができます。 これにより、モデルからの予測が不正に行われることがあります。\n",
    "\n",
    "Amazon SageMakerでは、モデル作成の対象がAWS Lambda、AWS Greengrass、Amazon Redshift、Amazon Athena、その他のデプロイメントターゲットであれば、モデルをインポートしないよう選択するだけでなく、他の場所で訓練されたモデルをインポートする柔軟性が得られます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor = fm.deploy(initial_instance_count=1,\n",
    "                         instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model for use\n",
    "Finally, we can now validate the model for use.  We can pass HTTP POST requests to the endpoint to get back predictions.  To make this easier, we'll again use the Amazon SageMaker Python SDK and specify how to serialize requests and deserialize responses that are specific to the algorithm.\n",
    "\n",
    "Since factorization machines are so frequently used with sparse data, making inference requests with a CSV format (as is done in other algorithm examples) can be massively inefficient.  Rather than waste space and time generating all of those zeros, to pad the row to the correct dimensionality, JSON can be used more efficiently.  Since we trained the model using dense data, this is a bit of a moot point, as we'll have to pass all the 0s in anyway.\n",
    "\n",
    "Nevertheless, we'll write our own small function to serialize our inference request in the JSON format that Amazon SageMaker Factorization Machines expects.\n",
    "\n",
    "---\n",
    "\n",
    "## 使用するためにモデルを検証する\n",
    "最後に、モデルを使用するために検証することができます。 エンドポイントにHTTP POST要求を渡すことで、予測を取り戻すことができます。 これを簡単にするために、Amazon SageMaker Python SDKを再度使用して、リクエストにシリアル化する方法と、アルゴリズムに固有のレスポンスを逆シリアル化する方法を指定します。\n",
    "\n",
    "Factorization Machineはスパースなデータで頻繁に使用されるので、（他のアルゴリズムの例で行われているように）CSV形式で推論要求を行うことは大いに非効率的である可能性があります。 スペースと時間を無駄にするのではなく、それらのゼロをすべて生成し、行を正しい次元に埋め込むために、JSONをより効率的に使用することができます。 私たちは密度の高いデータを使用してモデルを訓練してきたので、これはちょっとした問題です。なぜなら、とにかくすべての0を渡す必要があるからです。\n",
    "\n",
    "それにもかかわらず、私たちはAmazon SageMaker Factorization Machinesが期待するJSON形式で推論リクエストをシリアライズする独自の小さな関数を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.predictor import json_deserializer\n",
    "\n",
    "def fm_serializer(data):\n",
    "    js = {'instances': []}\n",
    "    for row in data:\n",
    "        js['instances'].append({'features': row.tolist()})\n",
    "    return json.dumps(js)\n",
    "\n",
    "fm_predictor.content_type = 'application/json'\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try getting a prediction for a single record.\n",
    "\n",
    "次に、1つのレコードの予測を試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fm_predictor.predict(train_set[0][30:31])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, a single prediction works.  We see that for one record our endpoint returned some JSON which contains `predictions`, including the `score` and `predicted_label`.  In this case, `score` will be a continuous value between [0, 1] representing the probability we think the digit is a 0 or not.  `predicted_label` will take a value of either `0` or `1` where (somewhat counterintuitively) `1` denotes that we predict the image is a 0, while `0` denotes that we are predicting the image is not of a 0.\n",
    "\n",
    "Let's do a whole batch of images and evaluate our predictive accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "OK、単一の予測が機能します。 1つのレコードに対して、エンドポイントは `score`と` predicted_label`を含む `predictions`を含むJSONをいくつか返しました。 この場合、「スコア」は、数字が0であると思われる確率を表す[0、1]の間の連続値になります。 `predicted_label`は` 0`または `1`の値をとるでしょう。（やや直感的に）` 1`は画像が0であることを意味し、 `0`は画像が0でないと予測していることを意味します 。\n",
    "\n",
    "一括して画像を作成し、予測精度を評価しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = []\n",
    "for array in np.array_split(test_set[0], 100):\n",
    "    result = fm_predictor.predict(array)\n",
    "    predictions += [r['predicted_label'] for r in result['predictions']]\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.crosstab(np.where(test_set[1] == 0, 1, 0), predictions, rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the confusion matrix above, we predict 951 images of the digit 0 correctly (confusingly this is class 1).  Meanwhile we predict 165 images as the digit 0 when in actuality they aren't, and we miss predicting 29 images of the digit 0 that we should have.\n",
    "\n",
    "*Note: Due to some differences in parameter initialization, your results may differ from those listed above, but should remain reasonably consistent.*\n",
    "\n",
    "---\n",
    "上記の混乱行列からわかるように、数字0の951個の画像を正しく予測します（これはクラス1です）。 一方、我々は、実際にはそうでない場合、数字0として165画像を予測し、数字0の29画像を予測しない。\n",
    "\n",
    "*注：パラメータの初期化の違いにより、上記の結果と異なる場合がありますが、合理的な一貫性を保つ必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Delete the Endpoint\n",
    "\n",
    "If you're ready to be done with this notebook, please run the delete_endpoint line in the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on.\n",
    "\n",
    "---\n",
    "\n",
    "### （オプション）エンドポイントを削除する\n",
    "\n",
    "このノートブックで作業する準備ができたら、下のセルでdelete_endpoint行を実行してください。 これにより、作成したホストされたエンドポイントが削除され、放置されているインスタンスからの料金は請求されません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker.Session().delete_endpoint(fm_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
