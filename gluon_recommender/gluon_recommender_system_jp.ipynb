{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker,MXNet,Gluonによるレコメンドシステムの実装\n",
    "_**Making Video Recommendations Using Neural Networks and Embeddings**_\n",
    "\n",
    "--- \n",
    "\n",
    "---\n",
    "\n",
    "*This work is based on content from the [Cyrus Vahid's 2017 re:Invent Talk](https://github.com/cyrusmvahid/gluontutorials/blob/master/recommendations/MLPMF.ipynb)*\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [背景](#Background)\n",
    "1. [セットアップ](#Setup)\n",
    "1. [データ](#Data)\n",
    "  1. [探索](#Explore)\n",
    "  1. [クリーニング](#Clean)\n",
    "  1. [前準備](#Prepare)\n",
    "1. [ローカルでのTraining](#Train-Locally)\n",
    "  1. [ネットワーク定義](#Define-Network)\n",
    "  1. [パラメータセット](#Set-Parameters)\n",
    "  1. [実行](#Execute)\n",
    "1. [SageMakerでのTraining](#Train-with-SageMaker)\n",
    "  1. [コード ラッピング](#Wrap-Code)\n",
    "  1. [データの移動](#Move-Data)\n",
    "  1. [提出](#Submit)\n",
    "1. [ホスティング](#Host)\n",
    "  1. [評価](#Evaluate)\n",
    "1. [ラップアップ](#Wrap-up)\n",
    "\n",
    "---\n",
    "\n",
    "## 背景\n",
    "\n",
    "In many ways, recommender systems were a catalyst for the current popularity of machine learning.  One of Amazon's earliest successes was the \"Customers who bought this, also bought...\" feature, while the million dollar Netflix Prize spurred research, raised public awareness, and inspired numerous other data science competitions.\n",
    "\n",
    "多くの点で、レコメンドシステムは、昨今における機械学習ブームの火付け役となりました。Amazonの最も早期の成功の1つは、「この商品を買ったお客様はこんなものを見ています」というレコメンドシステムでした。100万ドルのNetflix賞はRecommendationの研究を促し、一般の人々の意識を高め、多くのデータサイエンス大会に影響を与えました。\n",
    "\n",
    "Recommender systems can utilize a multitude of data sources and ML algorithms, and most combine various unsupervised, supervised, and reinforcement learning techniques into a holistic framework.  However, the core component is almost always a model which which predicts a user's rating (or purchase) for a certain item based on that user's historical ratings of similar items as well as the behavior of other similar users.  The minimal required dataset for this is a history of user item ratings.  In our case, we'll use 1 to 5 star ratings from over 2M Amazon customers on over 160K digital videos.  More details on this dataset can be found at its [AWS Public Datasets page](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "推薦システムは、多数のデータソースとMLアルゴリズムを利用することができ、多くの場合、教師あり、教師なし学習、強化学習等の、様々なテクニックを総合的なフレームワークに組み込みます。 ただし、コアコンポーネントは、ほとんどの場合、類似アイテムのそのユーザーの過去の評価および他の類似ユーザーの動作に基づいて、特定のアイテムのユーザーの評価（または購入）を予測するモデルです。 これに必要な最小限のデータセットは、ユーザーのアイテムレーティングの履歴です。 私たちの場合、160Kを超えるデジタルビデオで、2M以上のAmazonユーザーからの1から5の星評価を使用します。 このデータセットの詳細については、[AWS Public Datasets]ページ（https://s3.amazonaws.com/amazon-reviews-pds/readme.html）を参照してください。\n",
    "\n",
    "Matrix factorization has been the cornerstone of most user-item prediction models.  This method starts with the large, sparse, user-item ratings in a single matrix, where users index the rows, and items index the columns.  It then seeks to find two lower-dimensional, dense matrices which, when multiplied together, preserve the information and relationships in the larger matrix.\n",
    "\n",
    "![image](https://data-artisans.com/img/blog/factorization.svg)\n",
    "\n",
    "行列分解は、ほとんどのユーザ項目予測モデルの基礎となっています。 このメソッドは、ユーザーが行を索引付けし、項目が列を索引付けする単一のマトリックス内に、大規模でスパースなユーザー項目の格付けから始まります。 次に、2つの低次元の密行列を求め、それらを掛け合わせると、より大きな行列の情報と関係が保存されます。\n",
    "\n",
    "Matrix factorization has been extended and genarlized with deep learning and embeddings.  These techniques allows us to introduce non-linearities for enhanced performance and flexibility.  This notebook will fit a neural network-based model to generate recommendations for the Amazon video dataset.  It will start by exploring our data in the notebook and even training a model on a sample of the data.  Later we'll expand to the full dataset and fit our model using a SageMaker managed training cluster.  We'll then deploy to an endpoint and check our method.\n",
    "\n",
    "行列分解は、深い学習と埋め込みで拡張され、genarlizedされました。 これらの技術により、非線形性を導入してパフォーマンスと柔軟性を向上させることができます。 このノートブックは、ニューラルネットワークベースのモデルに適合し、Amazonビデオデータセットの推奨を生成します。 ノートブックのデータを調べたり、サンプルのデータをモデル化したりすることから始めます。 その後、SageMaker管理トレーニングクラスターを使用して完全なデータセットに拡張し、モデルに適合させます。 その後、エンドポイントに展開してメソッドをチェックします。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.p2.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `get_execution_role()` call with the appropriate full IAM role arn string(s).\n",
    "\n",
    "このノートブックは、ml.p2.xlargeノートブックインスタンスで作成およびテストされました。\n",
    "\n",
    "以下を指定して始めましょう。\n",
    "\n",
    "トレーニングとモデルデータに使用するS3バケットとプレフィックス。 これは、ノートブックインスタンス、トレーニング、およびホスティングと同じ領域内にある必要があります。\n",
    "IAMの役割は、トレーニングを行い、データへのアクセスをホストするために使用されます。 これらを作成する方法については、ドキュメントを参照してください。 ノートブックインスタンス、トレーニング、および/またはホスティングに複数のロールが必要な場合は、get_execution_role（）コールを適切な完全IAMロールのarnストリングに置き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-shokout'\n",
    "prefix = 'sagemaker/DEMO-gluon-recsys'\n",
    "\n",
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the Python libraries we'll need for the remainder of this example notebook.\n",
    "\n",
    "次に、このノートブックノートの残りの部分で必要となるPythonライブラリをロードしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, ndarray\n",
    "from mxnet.metric import MSE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "import boto3\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "### Explore\n",
    "\n",
    "Let's start by bringing in our dataset from an S3 public bucket.  As mentioned above, this contains 1 to 5 star ratings from over 2M Amazon customers on over 160K digital videos.  More details on this dataset can be found at its [AWS Public Datasets page](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).\n",
    "\n",
    "_Note, because this dataset is over a half gigabyte, the load from S3 may take ~10 minutes.  Also, since Amazon SageMaker Notebooks start with a 5GB persistent volume by default, and we don't need to keep this data on our instance for long, we'll bring it to the temporary volume (which has up to 20GB of storage)._\n",
    "\n",
    "まず、S3の公開バケットからデータセットを取り込みましょう。 上記のとおり、これには160,000以上のデジタルビデオ上の2M以上のAmazonユーザーからの1から5の星評価が含まれています。 このデータセットの詳細については、[AWS Public Datasets]ページ（https://s3.amazonaws.com/amazon-reviews-pds/readme.html）を参照してください。\n",
    "\n",
    "_注：このデータセットは半ギガバイトを超えているため、S3からの読み込みには約10分かかる場合があります。 また、Amazon SageMakerノートブックはデフォルトで5GBの永続的ボリュームから始まり、このデータをインスタンスに長く保存する必要はありません。一時的なボリューム（最大20GBのストレージ）に移動します。 _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/tmp/recsys/’: File exists\n",
      "download: s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz to ../../../../../tmp/recsys/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!mkdir /tmp/recsys/\n",
    "!aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz /tmp/recsys/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 --no-sign-request cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz /tmp/recsys/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data into a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) so that we can begin to understand it.\n",
    "\n",
    "*Note, we'll set `error_bad_lines=False` when reading the file in as there appear to be a very small number of records which would create a problem otherwise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 92523: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 343254: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 524626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 623024: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 977412: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1496867: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1711638: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1787213: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2395306: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2527690: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>12190288</td>\n",
       "      <td>R3FU16928EP5TC</td>\n",
       "      <td>B00AYB1482</td>\n",
       "      <td>668895143</td>\n",
       "      <td>Enlightened: Season 1</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>I loved it and I wish there was a season 3</td>\n",
       "      <td>I loved it and I wish there was a season 3... ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>30549954</td>\n",
       "      <td>R1IZHHS1MH3AQ4</td>\n",
       "      <td>B00KQD28OM</td>\n",
       "      <td>246219280</td>\n",
       "      <td>Vicious</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>As always it seems that the best shows come fr...</td>\n",
       "      <td>As always it seems that the best shows come fr...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>52895410</td>\n",
       "      <td>R52R85WC6TIAH</td>\n",
       "      <td>B01489L5LQ</td>\n",
       "      <td>534732318</td>\n",
       "      <td>After Words</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Charming movie</td>\n",
       "      <td>This movie isn't perfect, but it gets a lot of...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>27072354</td>\n",
       "      <td>R7HOOYTVIB0DS</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>239012694</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>excellant this is what tv should be</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>26939022</td>\n",
       "      <td>R1XQ2N5CDOZGNX</td>\n",
       "      <td>B0094LZMT0</td>\n",
       "      <td>535858974</td>\n",
       "      <td>On The Waterfront</td>\n",
       "      <td>Digital_Video_Download</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Brilliant film from beginning to end</td>\n",
       "      <td>Brilliant film from beginning to end. All of t...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     12190288  R3FU16928EP5TC  B00AYB1482       668895143   \n",
       "1          US     30549954  R1IZHHS1MH3AQ4  B00KQD28OM       246219280   \n",
       "2          US     52895410   R52R85WC6TIAH  B01489L5LQ       534732318   \n",
       "3          US     27072354   R7HOOYTVIB0DS  B008LOVIIK       239012694   \n",
       "4          US     26939022  R1XQ2N5CDOZGNX  B0094LZMT0       535858974   \n",
       "\n",
       "                           product_title        product_category  star_rating  \\\n",
       "0                  Enlightened: Season 1  Digital_Video_Download            5   \n",
       "1                                Vicious  Digital_Video_Download            5   \n",
       "2                            After Words  Digital_Video_Download            4   \n",
       "3  Masterpiece: Inspector Lewis Season 5  Digital_Video_Download            5   \n",
       "4                      On The Waterfront  Digital_Video_Download            5   \n",
       "\n",
       "   helpful_votes  total_votes vine verified_purchase  \\\n",
       "0              0            0    N                 Y   \n",
       "1              0            0    N                 Y   \n",
       "2             17           18    N                 Y   \n",
       "3              0            0    N                 Y   \n",
       "4              0            0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0         I loved it and I wish there was a season 3   \n",
       "1  As always it seems that the best shows come fr...   \n",
       "2                                     Charming movie   \n",
       "3                                         Five Stars   \n",
       "4               Brilliant film from beginning to end   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0  I loved it and I wish there was a season 3... ...  2015-08-31  \n",
       "1  As always it seems that the best shows come fr...  2015-08-31  \n",
       "2  This movie isn't perfect, but it gets a lot of...  2015-08-31  \n",
       "3                excellant this is what tv should be  2015-08-31  \n",
       "4  Brilliant film from beginning to end. All of t...  2015-08-31  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/tmp/recsys/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz', delimiter='\\t',error_bad_lines=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this dataset includes information like:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written.\n",
    "\n",
    "For this example, let's limit ourselves to `customer_id`, `product_id`, and `star_rating`.  Including additional features in our recommendation system could be beneficial, but would require substantial processing (particularly the text data) which would take us beyond the scope of this notebook.\n",
    "\n",
    "*Note: we'll keep `product_title` on the dataset to help verify our recommendations later in the notebook, but it will not be used in algorithm training.*\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "このデータセットには次のような情報が含まれています。\n",
    "\n",
    "- `marketplace`：2文字の国コード（この場合はすべて「US」）。\n",
    "- `customer_id`：単一の著者によって書かれたレビューを集約するために使用できるランダムな識別子。\n",
    "- `review_id`：レビューの一意のIDです。\n",
    "- `product_id`：Amazon標準識別番号（ASIN）です。 `http://www.amazon.com/dp/ <ASIN>`は製品の詳細ページにリンクしています。\n",
    "- `product_parent`：そのASINの親です。複数のASIN（同じ製品のカラーバリエーションまたはフォーマットバリエーション）を1つの親の親にロールアップすることができます。\n",
    "- `product_title`：製品のタイトルの説明。\n",
    "- 'product_category`：レビューをグループ化するために使用できる幅広い製品カテゴリ（この場合はデジタルビデオ）。\n",
    "- `star_rating`：レビューの評価（1〜5つ星）\n",
    "- `helpful_votes`：レビューの有益な投票数。\n",
    "- `total_votes`：レビューが受け取った総投票数。\n",
    "- `vine`：レビューは[Vine]（https://www.amazon.com/gp/vine/help）プログラムの一部として書かれましたか？\n",
    "- `verified_purchase`：確認された購入からのレビューですか？\n",
    "- `review_headline`：レビュー自体のタイトル。\n",
    "- `review_body`：レビューのテキスト。\n",
    "- `review_date`：レビューが書かれた日付。\n",
    "\n",
    "この例では、 `customer_id`、` product_id`、 `star_rating`に限定しましょう。推奨システムに追加機能を組み込むことは有益ですが、このノートブックの範囲を超えてしまうかなりの処理（特にテキストデータ）が必要になります。\n",
    "\n",
    "*ノート：後でノートブックの推奨事項を確認するために `product_title`をデータセットに保存しますが、アルゴリズムトレーニングには使用されません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['customer_id', 'product_id', 'star_rating', 'product_title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because most people haven't seen most videos, and people rate fewer videos than we actually watch, we'd expect our data to be sparse.  Our algorithm should work well with this sparse problem in general, but we may still want to clean out some of the long tail.  Let's look at some basic percentiles to confirm.\n",
    "\n",
    "ほとんどの人はほとんどの動画を見たことがなく、実際に見るよりも動画の割合が低いため、データが不足していると思われます。 私たちのアルゴリズムはこの一般的なスパースな問題にはうまくいくはずですが、まだ長いテールのいくつかを取り除きたいかもしれません。 確認するためにいくつかの基本的なパーセンタイルを見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers\n",
      " 0.00       1.0\n",
      "0.01       1.0\n",
      "0.02       1.0\n",
      "0.03       1.0\n",
      "0.04       1.0\n",
      "0.05       1.0\n",
      "0.10       1.0\n",
      "0.25       1.0\n",
      "0.50       1.0\n",
      "0.75       2.0\n",
      "0.90       4.0\n",
      "0.95       5.0\n",
      "0.96       6.0\n",
      "0.97       7.0\n",
      "0.98       9.0\n",
      "0.99      13.0\n",
      "1.00    2704.0\n",
      "Name: customer_id, dtype: float64\n",
      "products\n",
      " 0.00        1.00\n",
      "0.01        1.00\n",
      "0.02        1.00\n",
      "0.03        1.00\n",
      "0.04        1.00\n",
      "0.05        1.00\n",
      "0.10        1.00\n",
      "0.25        1.00\n",
      "0.50        3.00\n",
      "0.75        9.00\n",
      "0.90       31.00\n",
      "0.95       73.00\n",
      "0.96       95.00\n",
      "0.97      130.00\n",
      "0.98      199.00\n",
      "0.99      386.67\n",
      "1.00    32790.00\n",
      "Name: product_id, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "customers = df['customer_id'].value_counts()\n",
    "products = df['product_id'].value_counts()\n",
    "\n",
    "quantiles = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99, 1]\n",
    "print('customers\\n', customers.quantile(quantiles))\n",
    "print('products\\n', products.quantile(quantiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, only about 5% of customers have rated 5 or more videos, and only 25% of videos have been rated by 9+ customers.\n",
    "\n",
    "ご覧のように、5％以上の動画を評価したのは約5％に過ぎず、9％以上のお客様が動画の25％しか評価していません。\n",
    "\n",
    "### Clean\n",
    "\n",
    "Let's filter out this long tail.\n",
    "\n",
    "この長い尾を除外しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers[customers >= 5]\n",
    "products = products[products >= 10]\n",
    "\n",
    "reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll recreate our customer and product lists since there are customers with more than 5 reviews, but all of their reviews are on products with less than 5 reviews (and vice versa).\n",
    "\n",
    "今度は、5つ以上のレビューを持つ顧客が存在するため、顧客リストと商品リストを再作成しますが、すべてのレビューは5件未満のレビュー（およびその逆）の商品にあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = reduced_df['customer_id'].value_counts()\n",
    "products = reduced_df['product_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll number each user and item, giving them their own sequential index.  This will allow us to hold the information in a sparse format where the sequential indices indicate the row and column in our ratings matrix.\n",
    "\n",
    "次に、それぞれのユーザーとアイテムに番号を付け、それぞれに独自の順次インデックスを付けます。 これにより、私たちは、連続したインデックスが評価マトリックスの行と列を示す疎フォーマットで情報を保持することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>product_title</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27072354</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>5</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>10463</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16030865</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>5</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>489</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44025160</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>5</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>32100</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18602179</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>5</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>2237</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14424972</td>\n",
       "      <td>B008LOVIIK</td>\n",
       "      <td>5</td>\n",
       "      <td>Masterpiece: Inspector Lewis Season 5</td>\n",
       "      <td>32340</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id  star_rating  \\\n",
       "0     27072354  B008LOVIIK            5   \n",
       "1     16030865  B008LOVIIK            5   \n",
       "2     44025160  B008LOVIIK            5   \n",
       "3     18602179  B008LOVIIK            5   \n",
       "4     14424972  B008LOVIIK            5   \n",
       "\n",
       "                           product_title   user  item  \n",
       "0  Masterpiece: Inspector Lewis Season 5  10463   107  \n",
       "1  Masterpiece: Inspector Lewis Season 5    489   107  \n",
       "2  Masterpiece: Inspector Lewis Season 5  32100   107  \n",
       "3  Masterpiece: Inspector Lewis Season 5   2237   107  \n",
       "4  Masterpiece: Inspector Lewis Season 5  32340   107  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\n",
    "product_index = pd.DataFrame({'product_id': products.index, \n",
    "                              'item': np.arange(products.shape[0])})\n",
    "\n",
    "reduced_df = reduced_df.merge(customer_index).merge(product_index)\n",
    "reduced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備\n",
    "\n",
    "Let's start by splitting in training and test sets.  This will allow us to estimate the model's accuracy on videos our customers rated, but wasn't included in our training.\n",
    "\n",
    "トレーニングとテストセットを分割して始めましょう。 これにより、お客様が評価した動画のモデルの精度を見積もることができますが、トレーニングには含まれませんでした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reduced_df.groupby('customer_id').last().reset_index()\n",
    "\n",
    "train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \n",
    "                            on=['customer_id', 'product_id'], \n",
    "                            how='outer', \n",
    "                            indicator=True)\n",
    "train_df = train_df[(train_df['_merge'] == 'left_only')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll prepare for MXNet.  The data is sparse, so we'll first create a sparse matrix class that takes in our features (user and item indices) and our label (the star rating for that user-item combination).  This actually represents our data as a long list of (row index, column index, rating) values, rather than the large sparse user-item matrix shown above.  However, it doesn't change the mathematics of our model.\n",
    "\n",
    "MXNetの準備をします。 データは疎なので、最初に私たちの特徴（ユーザーとアイテムのインデックス）と私たちのラベル（そのユーザーアイテムの組み合わせの星の評価）を取り入れるスパース行列クラスを作成します。 これは実際に私たちのデータを上記の大きな疎なユーザ項目マトリックスではなく長いリスト（行インデックス、列インデックス、評価）の値として表しています。 ただし、モデルの数学は変更されません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMatrixDataset(gluon.data.Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        assert data.shape[0] == len(label)\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        if isinstance(label, ndarray.NDArray) and len(label.shape) == 1:\n",
    "            self._label = label.asnumpy()\n",
    "        else:\n",
    "            self._label = label       \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx, 0], self.data[idx, 1], self.label[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can convert our Pandas DataFrames into MXNet NDArrays, use those to create a member of the SparseMatrixDataset class, and add that to an MXNet Data Iterator.  This process is the same for both test and control.\n",
    "\n",
    "今度は、Pandas DataFramesをMXNet NDArraysに変換し、SparseMatrixDatasetクラスのメンバーを作成してMXNet Data Iteratorに追加します。 このプロセスは、テストとコントロールの両方で同じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40\n",
    "\n",
    "train_iter = gluon.data.DataLoader(SparseMatrixDataset(nd.array(train_df[['user', 'item']].values, dtype=np.float32), \n",
    "                                                       nd.array(train_df['star_rating'].values, dtype=np.float32)), \n",
    "                                   shuffle=True,\n",
    "                                   batch_size=batch_size)\n",
    "test_iter = gluon.data.DataLoader(SparseMatrixDataset(nd.array(test_df[['user', 'item']].values, dtype=np.float32), \n",
    "                                                      nd.array(test_df['star_rating'].values, dtype=np.float32)),\n",
    "                                  shuffle=True,\n",
    "                                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ローカルでTrainingする\n",
    "\n",
    "### Define Network\n",
    "\n",
    "Let's start by defining the neural network version of our matrix factorization task.  In this case, our network is quite simple.  The main components are:\n",
    "- [Embeddings](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Embedding) which turn our indexes into dense vectors of fixed size.  In this case, 64.\n",
    "- [Dense layers](https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dense) with ReLU activation.  Each dense layer has the same number of units as our number of embeddings.  Our ReLU activation here also adds some non-linearity to our matrix factorization.\n",
    "- [Dropout layers](https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dropout) which can be used to prevent over-fitting.\n",
    "- Matrix multiplication of our user matrix and our item matrix to create an estimate of our rating matrix.\n",
    "\n",
    "\n",
    "マトリックス分解タスクのニューラルネットワーク版を定義することから始めましょう。 この場合、私たちのネットワークは非常に簡単です。 主なコンポーネントは次のとおりです。\n",
    "- [Embeddings]（https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Embedding）インデックスを固定サイズの密ベクトルに変換します。 この場合、64。\n",
    "- ReLUを有効にした[Dense layers]（https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dense） 各高密度層は、我々の埋め込み数と同じ数のユニットを有する。 ここでのReLUの活性化は、行列分解に非線形性を追加します。\n",
    "- 過剰嵌合を防止するために使用できる[ドロップアウト層]（https://mxnet.incubator.apache.org/api/python/gluon.html#mxnet.gluon.nn.Dropout）。\n",
    "- ユーザマトリクスとアイテムマトリクスのマトリクス乗算により、評価マトリクスの見積もりを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFBlock(gluon.HybridBlock):\n",
    "    def __init__(self, max_users, max_items, num_emb, dropout_p=0.5):\n",
    "        super(MFBlock, self).__init__()\n",
    "        \n",
    "        self.max_users = max_users\n",
    "        self.max_items = max_items\n",
    "        self.dropout_p = dropout_p\n",
    "        self.num_emb = num_emb\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.user_embeddings = gluon.nn.Embedding(max_users, num_emb)\n",
    "            self.item_embeddings = gluon.nn.Embedding(max_items, num_emb)\n",
    "            self.dropout = gluon.nn.Dropout(dropout_p)\n",
    "            self.dense = gluon.nn.Dense(num_emb, activation='relu')\n",
    "            \n",
    "    def hybrid_forward(self, F, users, items):\n",
    "        a = self.user_embeddings(users)\n",
    "        a = self.dense(a)\n",
    "        \n",
    "        b = self.item_embeddings(items)\n",
    "        b = self.dense(b)\n",
    "\n",
    "        predictions = self.dropout(a) * self.dropout(b)      \n",
    "        predictions = F.sum(predictions, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 64\n",
    "\n",
    "net = MFBlock(max_users=customer_index.shape[0], \n",
    "              max_items=product_index.shape[0],\n",
    "              num_emb=num_embeddings,\n",
    "              dropout_p=0.)\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パラメータのセット\n",
    "\n",
    "Let's initialize network weights and set our optimization parameters.\n",
    "\n",
    "ネットワークの重みを初期化し、最適化パラメータをセットしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network parameters\n",
    "# ctx = mx.gpu()\n",
    "ctx = mx.cpu()\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24),\n",
    "                                ctx=ctx,\n",
    "                                force_reinit=True)\n",
    "net.hybridize()\n",
    "\n",
    "# Set optimization parameters\n",
    "opt = 'sgd'\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "wd = 0.\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        opt,\n",
    "                        {'learning_rate': lr,\n",
    "                         'wd': wd,\n",
    "                         'momentum': momentum})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute\n",
    "\n",
    "Let's define a function to carry out the training of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(train_iter, test_iter, net, epochs, ctx):\n",
    "    loss_function = gluon.loss.L2Loss()\n",
    "    for e in range(epochs):\n",
    "        print(\"epoch: {}\".format(e))\n",
    "        for i, (user, item, label) in enumerate(train_iter):\n",
    "            try:\n",
    "                user = user.as_in_context(ctx).reshape((batch_size,))\n",
    "                item = item.as_in_context(ctx).reshape((batch_size,))\n",
    "                label = label.as_in_context(ctx).reshape((batch_size,))\n",
    "                with mx.autograd.record():\n",
    "                    output = net(user, item)               \n",
    "                    loss = loss_function(output, label)\n",
    "                loss.backward()\n",
    "                trainer.step(batch_size)\n",
    "            except:\n",
    "                pass\n",
    "        print(\"EPOCH {}: MSE ON TRAINING and TEST: {}. {}\".format(e,\n",
    "                                                                   eval_net(train_iter, net, ctx, loss_function),\n",
    "                                                                   eval_net(test_iter, net, ctx, loss_function)))\n",
    "    print(\"end of training\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a function which evaluates our network on a given dataset.  This is called by our `execute` function above to provide mean squared error values on our training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(data, net, ctx, loss_function):\n",
    "    acc = MSE()\n",
    "    for i, (user, item, label) in enumerate(data):\n",
    "        try:\n",
    "            user = user.as_in_context(ctx).reshape((batch_size,))\n",
    "            item = item.as_in_context(ctx).reshape((batch_size,))\n",
    "            label = label.as_in_context(ctx).reshape((batch_size, 1))\n",
    "            predictions = net(user, item).reshape((batch_size, 1))\n",
    "            acc.update(preds=[predictions], labels=[label])\n",
    "        except:\n",
    "            pass\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "trained_net = execute(train_iter, test_iter, net, epochs, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Validation\n",
    "\n",
    "We can see our training error going down, but our validation accuracy bounces around a bit.  Let's check how our model is predicting for an individual user.  We could pick randomly, but for this case, let's try user #6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_index['u6_predictions'] = trained_net(nd.array([6] * product_index.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index['item'].values).as_in_context(ctx)).asnumpy()\n",
    "product_index.sort_values('u6_predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare this to the predictions for another user (we'll try user #7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_index['u7_predictions'] = trained_net(nd.array([7] * product_index.shape[0]).as_in_context(ctx), \n",
    "                                              nd.array(product_index['item'].values).as_in_context(ctx)).asnumpy()\n",
    "product_index.sort_values('u7_predictions', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted ratings are different between the two users, but the same top (and bottom) items for user #6 appear for #7 as well.  Let's look at the correlation across the full set of 38K items to see if this relationship holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_index[['u6_predictions', 'u7_predictions']].plot.scatter('u6_predictions', 'u7_predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this correlation is nearly perfect.  Essentially the average rating of items dominates across users and we'll recommend the same well-reviewed items to everyone.  As it turns out, we can add more embeddings and this relationship will go away since we're better able to capture differential preferences across users.\n",
    "\n",
    "However, with just a 64 dimensional embedding, it took 40 minutes to run just 3 epochs.  If we ran this outside of our Notebook Instance we could run larger jobs and move on to other work would improve productivity.\n",
    "\n",
    "---\n",
    "\n",
    "## Train with SageMaker\n",
    "\n",
    "Now that we've trained on this smaller dataset, we can expand training in SageMaker's distributed, managed training environment.\n",
    "\n",
    "### Wrap Code\n",
    "\n",
    "To use SageMaker's pre-built MXNet container, we'll need to wrap our code from above into a Python script.  There's a great deal of flexibility in using SageMaker's pre-built containers, and detailed documentation can be found [here](https://github.com/aws/sagemaker-python-sdk#mxnet-sagemaker-estimators), but for our example, it consisted of:\n",
    "1. Wrapping all data preparation into a `prepare_train_data` function (we could name this whatever we like)\n",
    "1. Copying and pasting classes and functions from above word-for-word\n",
    "1. Defining a `train` function that:\n",
    "  1. Adds a bit of new code to pick up the input TSV dataset on the SageMaker Training cluster\n",
    "  1. Takes in a dict of hyperparameters (which we specified as globals above)\n",
    "  1. Creates the net and executes training\n",
    "  \n",
    "---\n",
    "\n",
    "この相関関係はほぼ完璧であることがわかります。基本的にはアイテムの平均評価がユーザー全体を支配しており、全員によくレビューされたアイテムを推奨します。結果として、より多くの埋め込みを追加することができます。この関係は、ユーザー間の差別的な嗜好をよりよく捕えることができます。\n",
    "\n",
    "しかし、わずか64次元の埋め込みでは、わずか3エポックを実行するのに40分かかりました。これをNotebook Instanceの外で実行した場合、より大きな仕事を実行して他の仕事に進むことができ、生産性が向上します。\n",
    "\n",
    "---\n",
    "\n",
    "## SageMakerでトレーニングする\n",
    "\n",
    "この小さなデータセットで訓練したので、SageMakerの分散管理されたトレーニング環境でトレーニングを拡張することができます。\n",
    "\n",
    "###ラップコード\n",
    "\n",
    "SageMakerのあらかじめ構築されたMXNetコンテナを使用するには、上のコードをPythonスクリプトにラップする必要があります。 SageMakerの事前構築されたコンテナの使用には多大な柔軟性があり、詳細な文書はこちら（https://github.com/aws/sagemaker-python-sdk#mxnet-sagemaker-estimators）にありますが、例：\n",
    "1.すべてのデータ準備を `prepare_train_data`関数にラップします（私たちが好きなように名前をつけることができます）\n",
    "1.単語から単語までのクラスと関数のコピーと貼り付け\n",
    "1.次のような `train`関数を定義する：\n",
    "  1. SageMaker Trainingクラスタで入力TSVデータセットをピックアップするための新しいコードを少し追加します\n",
    "  1.ハイパーパラメータのdictを取る（これを上記のグローバルとして指定）\n",
    "  1.ネットを作成し、トレーニングを実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import logging\r\n",
      "import json\r\n",
      "import time\r\n",
      "import os\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, nd, ndarray\r\n",
      "from mxnet.metric import MSE\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "os.system('pip install pandas')\r\n",
      "import pandas as pd\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "#########\r\n",
      "# Globals\r\n",
      "#########\r\n",
      "\r\n",
      "batch_size = 40\r\n",
      "\r\n",
      "\r\n",
      "##########\r\n",
      "# Training\r\n",
      "##########\r\n",
      "\r\n",
      "def train(channel_input_dirs, hyperparameters, hosts, num_gpus, **kwargs):\r\n",
      "    \r\n",
      "    # get data\r\n",
      "    training_dir = channel_input_dirs['train']\r\n",
      "    train_iter, test_iter, customer_index, product_index = prepare_train_data(training_dir)\r\n",
      "    \r\n",
      "    # get hyperparameters\r\n",
      "    num_embeddings = hyperparameters.get('num_embeddings', 64)\r\n",
      "    opt = hyperparameters.get('opt', 'sgd')\r\n",
      "    lr = hyperparameters.get('lr', 0.02)\r\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\r\n",
      "    wd = hyperparameters.get('wd', 0.)\r\n",
      "    epochs = hyperparameters.get('epochs', 5)\r\n",
      "\r\n",
      "    # define net\r\n",
      "    ctx = mx.gpu()\r\n",
      "\r\n",
      "    net = MFBlock(max_users=customer_index.shape[0], \r\n",
      "                  max_items=product_index.shape[0],\r\n",
      "                  num_emb=num_embeddings,\r\n",
      "                  dropout_p=0.)\r\n",
      "    net.collect_params()\r\n",
      "    \r\n",
      "    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24),\r\n",
      "                                    ctx=ctx,\r\n",
      "                                    force_reinit=True)\r\n",
      "    net.hybridize()\r\n",
      "\r\n",
      "    trainer = gluon.Trainer(net.collect_params(),\r\n",
      "                            opt,\r\n",
      "                            {'learning_rate': lr,\r\n",
      "                             'wd': wd,\r\n",
      "                             'momentum': momentum})\r\n",
      "    \r\n",
      "    # execute\r\n",
      "    trained_net = execute(train_iter, test_iter, net, trainer, epochs, ctx)\r\n",
      "    \r\n",
      "    return trained_net, customer_index, product_index\r\n",
      "\r\n",
      "\r\n",
      "class MFBlock(gluon.HybridBlock):\r\n",
      "    def __init__(self, max_users, max_items, num_emb, dropout_p=0.5):\r\n",
      "        super(MFBlock, self).__init__()\r\n",
      "        \r\n",
      "        self.max_users = max_users\r\n",
      "        self.max_items = max_items\r\n",
      "        self.dropout_p = dropout_p\r\n",
      "        self.num_emb = num_emb\r\n",
      "        \r\n",
      "        with self.name_scope():\r\n",
      "            self.user_embeddings = gluon.nn.Embedding(max_users, num_emb)\r\n",
      "            self.item_embeddings = gluon.nn.Embedding(max_items, num_emb)\r\n",
      "            self.dropout = gluon.nn.Dropout(dropout_p)\r\n",
      "            self.dense = gluon.nn.Dense(num_emb, activation='relu')\r\n",
      "            \r\n",
      "    def hybrid_forward(self, F, users, items):\r\n",
      "        a = self.user_embeddings(users)\r\n",
      "        a = self.dense(a)\r\n",
      "        \r\n",
      "        b = self.item_embeddings(items)\r\n",
      "        b = self.dense(b)\r\n",
      "\r\n",
      "        predictions = self.dropout(a) * self.dropout(b)      \r\n",
      "        predictions = F.sum(predictions, axis=1)\r\n",
      "        return predictions\r\n",
      "\r\n",
      "    \r\n",
      "def execute(train_iter, test_iter, net, trainer, epochs, ctx):\r\n",
      "    loss_function = gluon.loss.L2Loss()\r\n",
      "    for e in range(epochs):\r\n",
      "        print(\"epoch: {}\".format(e))\r\n",
      "        for i, (user, item, label) in enumerate(train_iter):\r\n",
      "            try:\r\n",
      "                user = user.as_in_context(ctx).reshape((batch_size,))\r\n",
      "                item = item.as_in_context(ctx).reshape((batch_size,))\r\n",
      "                label = label.as_in_context(ctx).reshape((batch_size,))\r\n",
      "                with mx.autograd.record():\r\n",
      "                    output = net(user, item)               \r\n",
      "                    loss = loss_function(output, label)\r\n",
      "                loss.backward()\r\n",
      "                trainer.step(batch_size)\r\n",
      "            except:\r\n",
      "                pass\r\n",
      "        print(\"EPOCH {}: MSE ON TRAINING and TEST: {}. {}\".format(e,\r\n",
      "                                                                   eval_net(train_iter, net, ctx, loss_function),\r\n",
      "                                                                   eval_net(test_iter, net, ctx, loss_function)))\r\n",
      "    print(\"end of training\")\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def eval_net(data, net, ctx, loss_function):\r\n",
      "    acc = MSE()\r\n",
      "    for i, (user, item, label) in enumerate(data):\r\n",
      "        try:\r\n",
      "            user = user.as_in_context(ctx).reshape((batch_size,))\r\n",
      "            item = item.as_in_context(ctx).reshape((batch_size,))\r\n",
      "            label = label.as_in_context(ctx).reshape((batch_size, 1))\r\n",
      "            predictions = net(user, item).reshape((batch_size, 1))\r\n",
      "            acc.update(preds=[predictions], labels=[label])\r\n",
      "        except:\r\n",
      "            pass\r\n",
      "    return acc.get()[1]\r\n",
      "\r\n",
      "\r\n",
      "def save(model, model_dir):\r\n",
      "    net, customer_index, product_index = model\r\n",
      "    net.save_params('{}/model.params'.format(model_dir))\r\n",
      "    f = open('{}/MFBlock.params'.format(model_dir), 'w')\r\n",
      "    json.dump({'max_users': net.max_users,\r\n",
      "               'max_items': net.max_items,\r\n",
      "               'num_emb': net.num_emb,\r\n",
      "               'dropout_p': net.dropout_p},\r\n",
      "              f)\r\n",
      "    f.close()\r\n",
      "    customer_index.to_csv('{}/customer_index.csv'.format(model_dir), index=False)\r\n",
      "    product_index.to_csv('{}/product_index.csv'.format(model_dir), index=False)\r\n",
      "\r\n",
      "    \r\n",
      "######\r\n",
      "# Data\r\n",
      "######\r\n",
      "\r\n",
      "class SparseMatrixDataset(gluon.data.Dataset):\r\n",
      "    def __init__(self, data, label):\r\n",
      "        assert data.shape[0] == len(label)\r\n",
      "        self.data = data\r\n",
      "        self.label = label\r\n",
      "        if isinstance(label, ndarray.NDArray) and len(label.shape) == 1:\r\n",
      "            self._label = label.asnumpy()\r\n",
      "        else:\r\n",
      "            self._label = label       \r\n",
      "        \r\n",
      "    def __getitem__(self, idx):\r\n",
      "        return self.data[idx, 0], self.data[idx, 1], self.label[idx]\r\n",
      "    \r\n",
      "    def __len__(self):\r\n",
      "        return self.data.shape[0]        \r\n",
      "\r\n",
      "    \r\n",
      "def prepare_train_data(training_dir):\r\n",
      "    f = os.listdir(training_dir)\r\n",
      "    df = pd.read_csv(os.path.join(training_dir, f[0]), delimiter='\\t', error_bad_lines=False)\r\n",
      "    df = df[['customer_id', 'product_id', 'star_rating']]\r\n",
      "    customers = df['customer_id'].value_counts()\r\n",
      "    products = df['product_id'].value_counts()\r\n",
      "    \r\n",
      "    # Filter long-tail\r\n",
      "    customers = customers[customers >= 5]\r\n",
      "    products = products[products >= 10]\r\n",
      "\r\n",
      "    reduced_df = df.merge(pd.DataFrame({'customer_id': customers.index})).merge(pd.DataFrame({'product_id': products.index}))\r\n",
      "    customers = reduced_df['customer_id'].value_counts()\r\n",
      "    products = reduced_df['product_id'].value_counts()\r\n",
      "\r\n",
      "    # Number users and items\r\n",
      "    customer_index = pd.DataFrame({'customer_id': customers.index, 'user': np.arange(customers.shape[0])})\r\n",
      "    product_index = pd.DataFrame({'product_id': products.index, 'item': np.arange(products.shape[0])})\r\n",
      "\r\n",
      "    reduced_df = reduced_df.merge(customer_index).merge(product_index)\r\n",
      "\r\n",
      "    # Split train and test\r\n",
      "    test_df = reduced_df.groupby('customer_id').last().reset_index()\r\n",
      "\r\n",
      "    train_df = reduced_df.merge(test_df[['customer_id', 'product_id']], \r\n",
      "                                on=['customer_id', 'product_id'], \r\n",
      "                                how='outer', \r\n",
      "                                indicator=True)\r\n",
      "    train_df = train_df[(train_df['_merge'] == 'left_only')]\r\n",
      "\r\n",
      "    # MXNet data iterators\r\n",
      "    train_iter = gluon.data.DataLoader(SparseMatrixDataset(nd.array(train_df[['user', 'item']].values, dtype=np.float32), \r\n",
      "                                                           nd.array(train_df['star_rating'].values, dtype=np.float32)), \r\n",
      "                                       shuffle=True,\r\n",
      "                                       batch_size=batch_size)\r\n",
      "    test_iter = gluon.data.DataLoader(SparseMatrixDataset(nd.array(test_df[['user', 'item']].values, dtype=np.float32), \r\n",
      "                                                          nd.array(test_df['star_rating'].values, dtype=np.float32)),\r\n",
      "                                      shuffle=True,\r\n",
      "                                      batch_size=batch_size)\r\n",
      "\r\n",
      "    return train_iter, test_iter, customer_index, product_index \r\n",
      "\r\n",
      "\r\n",
      "#########\r\n",
      "# Hosting\r\n",
      "#########\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    \"\"\"\r\n",
      "    Load the gluon model. Called once when hosting service starts.\r\n",
      "\r\n",
      "    :param: model_dir The directory where model files are stored.\r\n",
      "    :return: a model (in this case a Gluon network)\r\n",
      "    \"\"\"\r\n",
      "    ctx = mx.cpu()\r\n",
      "    f = open('{}/MFBlock.params'.format(model_dir), 'r')\r\n",
      "    block_params = json.load(f)\r\n",
      "    f.close()\r\n",
      "    net = MFBlock(max_users=block_params['max_users'], \r\n",
      "                  max_items=block_params['max_items'],\r\n",
      "                  num_emb=block_params['num_emb'],\r\n",
      "                  dropout_p=block_params['dropout_p'])\r\n",
      "    net.load_params('{}/model.params'.format(model_dir), ctx)\r\n",
      "    customer_index = pd.read_csv('{}/customer_index.csv'.format(model_dir))\r\n",
      "    product_index = pd.read_csv('{}/product_index.csv'.format(model_dir))\r\n",
      "    return net, customer_index, product_index\r\n",
      "\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    \"\"\"\r\n",
      "    Transform a request using the Gluon model. Called once per request.\r\n",
      "\r\n",
      "    :param net: The Gluon model.\r\n",
      "    :param data: The request payload.\r\n",
      "    :param input_content_type: The request content type.\r\n",
      "    :param output_content_type: The (desired) response content type.\r\n",
      "    :return: response payload and content type.\r\n",
      "    \"\"\"\r\n",
      "    ctx = mx.cpu()\r\n",
      "    parsed = json.loads(data)\r\n",
      "\r\n",
      "    trained_net, customer_index, product_index = net\r\n",
      "    users = pd.DataFrame({'customer_id': parsed['customer_id']}).merge(customer_index, how='left')['user'].values\r\n",
      "    items = pd.DataFrame({'product_id': parsed['product_id']}).merge(product_index, how='left')['item'].values\r\n",
      "    \r\n",
      "    predictions = trained_net(nd.array(users).as_in_context(ctx), nd.array(items).as_in_context(ctx))\r\n",
      "    response_body = json.dumps(predictions.asnumpy().tolist())\r\n",
      "\r\n",
      "    return response_body, output_content_type\r\n"
     ]
    }
   ],
   "source": [
    "!cat recommender.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Locally\n",
    "\n",
    "Now we can test our train function locally.  This helps ensure we don't have any bugs before submitting our code to SageMaker's pre-built MXNet container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# import recommender\n",
    "\n",
    "# local_test_net, local_customer_index, local_product_index = recommender.train(\n",
    "#     {'train': '/tmp/recsys/'}, \n",
    "#     {'num_embeddings': 64, \n",
    "#      'opt': 'sgd', \n",
    "#      'lr': 0.02, \n",
    "#      'momentum': 0.9, \n",
    "#      'wd': 0.,\n",
    "#      'epochs': 3},\n",
    "#     ['local'],\n",
    "#     1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move Data\n",
    "\n",
    "Holding our data in memory works fine when we're interactively exploring a sample of data, but for larger, longer running processes, we'd prfer to run them in the background with SageMaker Training.  To do this, let's move the dataset to S3 so that it can be picked up by SageMaker training.  This is perfect for use cases like periodic re-training, expanding to a larger dataset, or moving production workloads to larger hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.client('s3').copy({'Bucket': 'amazon-reviews-pds', \n",
    "                         'Key': 'tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz'},\n",
    "                        bucket,\n",
    "                        prefix + '/train/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimization parameters\n",
    "opt = 'sgd'\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "wd = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit\n",
    "\n",
    "Now, we can create an MXNet estimator from the SageMaker Python SDK.  To do so, we need to pass in:\n",
    "1. Instance type and count for our SageMaker Training cluster.  SageMaker's MXNet containers support distributed GPU training, so we could easily set this to multiple ml.p2 or ml.p3 instances if we wanted.\n",
    "  - *Note, this would require some changes to our recommender.py script as we would need to setup the context an key value store properly, as well as determining if and how to distribute the training data.*\n",
    "1. An S3 path for out model artifacts and a role with access to S3 input and output paths.\n",
    "1. Hyperparameters for our neural network.  Since with a 64 dimensional embedding, our recommendations reverted too closely to the mean, let's increase this by an order of magnitude when we train outside of our local instance.  We'll also increase the epochs to see how our accuracy evolves over time. We'll leave all other hyperparameters the same.\n",
    "\n",
    "Once we use `.fit()` this creates a SageMaker Training Job that spins up instances, loads the appropriate packages and data, runs our `train` function from `recommender.py`, wraps up and saves model artifacts to S3, and finishes by tearing down the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-314676777416\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-2018-10-16-23-51-48-473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-16 23:51:48 Starting - Starting the training job...\n",
      "Launching requested ML instances......\n",
      "Preparing the instances for training......\n",
      "2018-10-16 23:53:55 Downloading - Downloading input data\n",
      "2018-10-16 23:54:08 Training - Downloading the training image...\n",
      "Training image download completed. Training in progress..\n",
      "\u001b[31m2018-10-16 23:54:47,523 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-10-16 23:54:47,523 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-10-16 23:54:47,580 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31m2018-10-16 23:54:48,880 WARNING - mxnet_container.train - #033[1;33mThis required structure for training scripts will be deprecated with the next major release of MXNet images. The train() function will no longer be required; instead the training script must be able to be run as a standalone script. For more information, see https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/mxnet#updating-your-mxnet-training-script.#033[1;0m\u001b[0m\n",
      "\u001b[31m2018-10-16 23:54:48,920 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'hyperparameters': {'sagemaker_container_log_level': 20, 'lr': 0.02, 'sagemaker_submit_directory': 's3://sagemaker-us-east-1-314676777416/sagemaker-mxnet-2018-10-16-23-51-48-473/source/sourcedir.tar.gz', 'num_embeddings': 512, 'opt': 'sgd', 'sagemaker_enable_cloudwatch_metrics': False, 'sagemaker_program': 'recommender.py', 'sagemaker_region': 'us-east-1', 'momentum': 0.9, 'wd': 0.0, 'sagemaker_job_name': 'sagemaker-mxnet-2018-10-16-23-51-48-473', 'epochs': 10}, 'hosts': ['algo-1'], 'channel_dirs': {'train': '/opt/ml/input/data/train'}, 'enable_cloudwatch_metrics': False, 'code_dir': '/opt/ml/code', 'sagemaker_region': 'us-east-1', 'user_script_name': 'recommender.py', 'base_dir': '/opt/ml', '_scheduler_ip': '10.32.0.4', 'input_config_dir': '/opt/ml/input/config', 'input_dir': '/opt/ml/input', 'available_cpus': 32, 'job_name': 'sagemaker-mxnet-2018-10-16-23-51-48-473', 'user_requirements_file': None, 'channels': {'train': {'TrainingInputMode': 'File', 'RecordWrapperType': 'None', 'S3DistributionType': 'FullyReplicated'}}, '_ps_verbose': 0, 'output_data_dir': '/opt/ml/output/data/', '_ps_port': 8000, '_scheduler_host': 'algo-1', 'model_dir': '/opt/ml/model', 'current_host': 'algo-1', 'output_dir': '/opt/ml/output', 'container_log_level': 20, 'available_gpus': 4, 'resource_config': {'hosts': ['algo-1'], 'current_host': 'algo-1', 'network_interface_name': 'ethwe'}, 'user_script_archive': 's3://sagemaker-us-east-1-314676777416/sagemaker-mxnet-2018-10-16-23-51-48-473/source/sourcedir.tar.gz'}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-east-1-314676777416/sagemaker-mxnet-2018-10-16-23-51-48-473/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-10-16 23:54:49,205 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[31mCollecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/5d/d4/6e9c56a561f1d27407bf29318ca43f36ccaa289271b805a30034eb3a8ec4/pandas-0.23.4-cp35-cp35m-manylinux1_x86_64.whl (8.7MB)\u001b[0m\n",
      "\u001b[31mCollecting pytz>=2011k (from pandas)\u001b[0m\n",
      "\u001b[31m  Downloading https://files.pythonhosted.org/packages/30/4e/27c34b62430286c6d59177a0842ed90dc789ce5d1ed740887653b898779a/pytz-2018.5-py2.py3-none-any.whl (510kB)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.14.6)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.7.3)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\u001b[0m\n",
      "\u001b[31mInstalling collected packages: pytz, pandas\u001b[0m\n",
      "\u001b[31mSuccessfully installed pandas-0.23.4 pytz-2018.5\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.5/dist-packages/mxnet_container/train.py:190: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  train_args = inspect.getargspec(user_module.train)\u001b[0m\n",
      "\u001b[31mb'Skipping line 92523: expected 15 fields, saw 22\\n'\u001b[0m\n",
      "\u001b[31mb'Skipping line 343254: expected 15 fields, saw 22\\n'\u001b[0m\n",
      "\u001b[31mb'Skipping line 524626: expected 15 fields, saw 22\\n'\u001b[0m\n",
      "\u001b[31mb'Skipping line 623024: expected 15 fields, saw 22\\n'\u001b[0m\n",
      "\u001b[31mb'Skipping line 977412: expected 15 fields, saw 22\\n'\u001b[0m\n",
      "\u001b[31mb'Skipping line 1496867: expected 15 fields, saw 22\\n'\u001b[0m\n",
      "\u001b[31mb'Skipping line 1711638: expected 15 fields, saw 22\\n'\u001b[0m\n",
      "\u001b[31mb'Skipping line 1787213: expected 15 fields, saw 22\\n'\u001b[0m\n",
      "\u001b[31mb'Skipping line 2395306: expected 15 fields, saw 22\\n'\u001b[0m\n",
      "\u001b[31mb'Skipping line 2527690: expected 15 fields, saw 22\\n'\u001b[0m\n",
      "\u001b[31mepoch: 0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "m = MXNet('recommender.py', \n",
    "          py_version='py3',\n",
    "          role=role, \n",
    "          train_instance_count=1, \n",
    "          train_instance_type=\"ml.p3.8xlarge\",\n",
    "          output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "          hyperparameters={'num_embeddings': 512, \n",
    "                           'opt': opt, \n",
    "                           'lr': lr, \n",
    "                           'momentum': momentum, \n",
    "                           'wd': wd,\n",
    "                           'epochs': 10})\n",
    "\n",
    "m.fit({'train': 's3://{}/{}/train/'.format(bucket, prefix)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Host\n",
    "\n",
    "Now that we've trained our model, deploying it to a real-time, production endpoint is easy.\n",
    "\n",
    "---\n",
    "\n",
    "＃＃ ホスト\n",
    "\n",
    "モデルを訓練したので、リアルタイムのプロダクションエンドポイントに導入するのは簡単です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = m.deploy(initial_instance_count=1, \n",
    "                     instance_type='ml.m4.xlarge')\n",
    "predictor.serializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an endpoint, let's test it out.  We'll predict user #6's ratings for the top and bottom ASINs from our local model.\n",
    "\n",
    "*This could be done by sending HTTP POST requests from a separate web service, but to keep things easy, we'll just use the `.predict()` method from the SageMaker Python SDK.*\n",
    "\n",
    "エンドポイントが完成したので、テストしましょう。 私たちは、ローカルモデルからトップ6と下部ASINのユーザー＃6の評価を予測します。\n",
    "\n",
    "*これは別のWebサービスからのHTTP POSTリクエストを送信することで行うことができますが、簡単にするために、SageMaker Python SDKの `.predict（）`メソッドを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 6]['customer_id'].values.tolist(), \n",
    "                              'product_id': ['B00KH1O9HW', 'B00M5KODWO']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note, some of our predictions are actually greater than 5, which is to be expected as we didn't do anything special to account for ratings being capped at that value.  Since we are only looking to ranking by predicted rating, this won't create problems for our specific use case.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Let's start by calculating a naive baseline to approximate how well our model is doing.  The simplest estimate would be to assume every user item rating is just the average rating over all ratings.\n",
    "\n",
    "*Note, we could do better by using each individual video's average, however, in this case it doesn't really matter as the same conclusions would hold.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive MSE:', np.mean((test_df['star_rating'] - np.mean(train_df['star_rating'])) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll calculate predictions for our test dataset.\n",
    "\n",
    "*Note, this will align closely to our CloudWatch output above, but may differ slightly due to skipping partial mini-batches in our eval_net function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for array in np.array_split(test_df[['customer_id', 'product_id']].values, 40):\n",
    "    test_preds += predictor.predict(json.dumps({'customer_id': array[:, 0].tolist(), \n",
    "                                                'product_id': array[:, 1].tolist()}))\n",
    "\n",
    "test_preds = np.array(test_preds)\n",
    "print('MSE:', np.mean((test_df['star_rating'] - test_preds) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our neural network and embedding model produces substantially better results (~1.27 vs 1.65 on mean square error).\n",
    "\n",
    "For recommender systems, subjective accuracy also matters.  Let's get some recommendations for a random user to see if they make intuitive sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df[reduced_df['user'] == 6].sort_values(['star_rating', 'item'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, user #6 seems to like sprawling dramamtic television series and sci-fi, but they dislike silly comedies.\n",
    "\n",
    "Now we'll loop through and predict user #6's ratings for every common video in the catalog, to see which ones we'd recommend and which ones we wouldn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for array in np.array_split(product_index['product_id'].values, 40):\n",
    "    predictions += predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 6]['customer_id'].values.tolist() * array.shape[0], \n",
    "                                                 'product_id': array.tolist()}))\n",
    "\n",
    "predictions = pd.DataFrame({'product_id': product_index['product_id'],\n",
    "                            'prediction': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = reduced_df.groupby('product_id')['product_title'].last().reset_index()\n",
    "predictions_titles = predictions.merge(titles)\n",
    "predictions_titles.sort_values(['prediction', 'product_id'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, our predicted highly rated shows have some well-reviewed TV dramas and some sci-fi.  Meanwhile, our bottom rated shows include goofball comedies.\n",
    "\n",
    "*Note, because of random initialization in the weights, results on subsequent runs may differ slightly.*\n",
    "\n",
    "Let's confirm that we no longer have almost perfect correlation in recommendations with user #7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_user7 = []\n",
    "for array in np.array_split(product_index['product_id'].values, 40):\n",
    "    predictions_user7 += predictor.predict(json.dumps({'customer_id': customer_index[customer_index['user'] == 7]['customer_id'].values.tolist() * array.shape[0], \n",
    "                                                       'product_id': array.tolist()}))\n",
    "plt.scatter(predictions['prediction'], np.array(predictions_user7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Wrap-up\n",
    "\n",
    "In this example, we developed a deep learning model to predict customer ratings.  This could serve as the foundation of a recommender system in a variety of use cases.  However, there are many ways in which it could be improved.  For example we did very little with:\n",
    "- hyperparameter tuning\n",
    "- controlling for overfitting (early stopping, dropout, etc.)\n",
    "- testing whether binarizing our target variable would improve results\n",
    "- including other information sources (video genres, historical ratings, time of review)\n",
    "- adjusting our threshold for user and item inclusion \n",
    "\n",
    "In addition to improving the model, we could improve the engineering by:\n",
    "- Setting the context and key value store up for distributed training\n",
    "- Fine tuning our data ingestion (e.g. num_workers on our data iterators) to ensure we're fully utilizing our GPU\n",
    "- Thinking about how pre-processing would need to change as datasets scale beyond a single machine\n",
    "\n",
    "Beyond that, recommenders are a very active area of research and techniques from active learning, reinforcement learning, segmentation, ensembling, and more should be investigated to deliver well-rounded recommendations.\n",
    "\n",
    "### Clean-up (optional)\n",
    "\n",
    "Let's finish by deleting our endpoint to avoid stray hosting charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
